{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNAozH4edT89"
   },
   "source": [
    "## 第7章: 単語ベクトル\n",
    "単語の意味を実ベクトルで表現する単語ベクトル（単語埋め込み）に関して，以下の処理を行うプログラムを作成せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sloq_VnCdY3b"
   },
   "source": [
    "#### 60. 単語ベクトルの読み込みと表示\n",
    "Google Newsデータセット（約1,000億単語）での学習済み単語ベクトル（300万単語・フレーズ，300次元）をダウンロードし，”United States”の単語ベクトルを表示せよ．ただし，”United States”は内部的には”United_States”と表現されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peIAuDa9dePS"
   },
   "source": [
    "指定の学習済み単語ベクトルをダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13075,
     "status": "ok",
     "timestamp": 1610700620736,
     "user": {
      "displayName": "yuki kato",
      "photoUrl": "",
      "userId": "08430221927587521076"
     },
     "user_tz": -540
    },
    "id": "FH79kfLGdM19",
    "outputId": "23e83db8-7c33-4f14-a3b7-2b1e8735019e"
   },
   "outputs": [],
   "source": [
    "# FILE_ID = \"0B7XkCwpI5KDYNlNUTTlSS21pQmM\"\n",
    "# FILE_NAME = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "# !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=$FILE_ID' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=$FILE_ID\" -O $FILE_NAME && rm -rf /tmp/cookies.txt\n",
    "\n",
    "# 下記からダウンロードできます。\n",
    "# https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8jxLHEFdjrZ"
   },
   "source": [
    "自然言語処理のさまざまなタスクで利用されるGensimを用いて、単語ベクトルを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 110605,
     "status": "ok",
     "timestamp": 1610700732778,
     "user": {
      "displayName": "yuki kato",
      "photoUrl": "",
      "userId": "08430221927587521076"
     },
     "user_tz": -540
    },
    "id": "EU3Ep67NdgvM"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(\n",
    "    \"./GoogleNews-vectors-negative300.bin.gz\", binary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9u9Hujaqdo0i"
   },
   "source": [
    "読み込んだ後は、ベクトル化したい単語を指定するだけで簡単に単語ベクトルを得ることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 681,
     "status": "ok",
     "timestamp": 1610700768218,
     "user": {
      "displayName": "yuki kato",
      "photoUrl": "",
      "userId": "08430221927587521076"
     },
     "user_tz": -540
    },
    "id": "IrEjL1XGdlUl",
    "outputId": "e343f229-52a2-45ff-ded7-dc58fafdbb44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.61328125e-02, -4.83398438e-02,  2.35351562e-01,  1.74804688e-01,\n",
       "       -1.46484375e-01, -7.42187500e-02, -1.01562500e-01, -7.71484375e-02,\n",
       "        1.09375000e-01, -5.71289062e-02, -1.48437500e-01, -6.00585938e-02,\n",
       "        1.74804688e-01, -7.71484375e-02,  2.58789062e-02, -7.66601562e-02,\n",
       "       -3.80859375e-02,  1.35742188e-01,  3.75976562e-02, -4.19921875e-02,\n",
       "       -3.56445312e-02,  5.34667969e-02,  3.68118286e-04, -1.66992188e-01,\n",
       "       -1.17187500e-01,  1.41601562e-01, -1.69921875e-01, -6.49414062e-02,\n",
       "       -1.66992188e-01,  1.00585938e-01,  1.15722656e-01, -2.18750000e-01,\n",
       "       -9.86328125e-02, -2.56347656e-02,  1.23046875e-01, -3.54003906e-02,\n",
       "       -1.58203125e-01, -1.60156250e-01,  2.94189453e-02,  8.15429688e-02,\n",
       "        6.88476562e-02,  1.87500000e-01,  6.49414062e-02,  1.15234375e-01,\n",
       "       -2.27050781e-02,  3.32031250e-01, -3.27148438e-02,  1.77734375e-01,\n",
       "       -2.08007812e-01,  4.54101562e-02, -1.23901367e-02,  1.19628906e-01,\n",
       "        7.44628906e-03, -9.03320312e-03,  1.14257812e-01,  1.69921875e-01,\n",
       "       -2.38281250e-01, -2.79541016e-02, -1.21093750e-01,  2.47802734e-02,\n",
       "        7.71484375e-02, -2.81982422e-02, -4.71191406e-02,  1.78222656e-02,\n",
       "       -1.23046875e-01, -5.32226562e-02,  2.68554688e-02, -3.11279297e-02,\n",
       "       -5.59082031e-02, -5.00488281e-02, -3.73535156e-02,  1.25976562e-01,\n",
       "        5.61523438e-02,  1.51367188e-01,  4.29687500e-02, -2.08007812e-01,\n",
       "       -4.78515625e-02,  2.78320312e-02,  1.81640625e-01,  2.20703125e-01,\n",
       "       -3.61328125e-02, -8.39843750e-02, -3.69548798e-05, -9.52148438e-02,\n",
       "       -1.25000000e-01, -1.95312500e-01, -1.50390625e-01, -4.15039062e-02,\n",
       "        1.31835938e-01,  1.17675781e-01,  1.91650391e-02,  5.51757812e-02,\n",
       "       -9.42382812e-02, -1.08886719e-01,  7.32421875e-02, -1.15234375e-01,\n",
       "        8.93554688e-02, -1.40625000e-01,  1.45507812e-01,  4.49218750e-02,\n",
       "       -1.10473633e-02, -1.62353516e-02,  4.05883789e-03,  3.75976562e-02,\n",
       "       -6.98242188e-02, -5.46875000e-02,  2.17285156e-02, -9.47265625e-02,\n",
       "        4.24804688e-02,  1.81884766e-02, -1.73339844e-02,  4.63867188e-02,\n",
       "       -1.42578125e-01,  1.99218750e-01,  1.10839844e-01,  2.58789062e-02,\n",
       "       -7.08007812e-02, -5.54199219e-02,  3.45703125e-01,  1.61132812e-01,\n",
       "       -2.44140625e-01, -2.59765625e-01, -9.71679688e-02,  8.00781250e-02,\n",
       "       -8.78906250e-02, -7.22656250e-02,  1.42578125e-01, -8.54492188e-02,\n",
       "       -3.18359375e-01,  8.30078125e-02,  6.34765625e-02,  1.64062500e-01,\n",
       "       -1.92382812e-01, -1.17675781e-01, -5.41992188e-02, -1.56250000e-01,\n",
       "       -1.21582031e-01, -4.95605469e-02,  1.20117188e-01, -3.83300781e-02,\n",
       "        5.51757812e-02, -8.97216797e-03,  4.32128906e-02,  6.93359375e-02,\n",
       "        8.93554688e-02,  2.53906250e-01,  1.65039062e-01,  1.64062500e-01,\n",
       "       -1.41601562e-01,  4.58984375e-02,  1.97265625e-01, -8.98437500e-02,\n",
       "        3.90625000e-02, -1.51367188e-01, -8.60595703e-03, -1.17675781e-01,\n",
       "       -1.97265625e-01, -1.12792969e-01,  1.29882812e-01,  1.96289062e-01,\n",
       "        1.56402588e-03,  3.93066406e-02,  2.17773438e-01, -1.43554688e-01,\n",
       "        6.03027344e-02, -1.35742188e-01,  1.16210938e-01, -1.59912109e-02,\n",
       "        2.79296875e-01,  1.46484375e-01, -1.19628906e-01,  1.76757812e-01,\n",
       "        1.28906250e-01, -1.49414062e-01,  6.93359375e-02, -1.72851562e-01,\n",
       "        9.22851562e-02,  1.33056641e-02, -2.00195312e-01, -9.76562500e-02,\n",
       "       -1.65039062e-01, -2.46093750e-01, -2.35595703e-02, -2.11914062e-01,\n",
       "        1.84570312e-01, -1.85546875e-02,  2.16796875e-01,  5.05371094e-02,\n",
       "        2.02636719e-02,  4.25781250e-01,  1.28906250e-01, -2.77099609e-02,\n",
       "        1.29882812e-01, -1.15722656e-01, -2.05078125e-02,  1.49414062e-01,\n",
       "        7.81250000e-03, -2.05078125e-01, -8.05664062e-02, -2.67578125e-01,\n",
       "       -2.29492188e-02, -8.20312500e-02,  8.64257812e-02,  7.61718750e-02,\n",
       "       -3.66210938e-02,  5.22460938e-02, -1.22070312e-01, -1.44042969e-02,\n",
       "       -2.69531250e-01,  8.44726562e-02, -2.52685547e-02, -2.96630859e-02,\n",
       "       -1.68945312e-01,  1.93359375e-01, -1.08398438e-01,  1.94091797e-02,\n",
       "       -1.80664062e-01,  1.93359375e-01, -7.08007812e-02,  5.85937500e-02,\n",
       "       -1.01562500e-01, -1.31835938e-01,  7.51953125e-02, -7.66601562e-02,\n",
       "        3.37219238e-03, -8.59375000e-02,  1.25000000e-01,  2.92968750e-02,\n",
       "        1.70898438e-01, -9.37500000e-02, -1.09375000e-01, -2.50244141e-02,\n",
       "        2.11914062e-01, -4.44335938e-02,  6.12792969e-02,  2.62451172e-02,\n",
       "       -1.77734375e-01,  1.23046875e-01, -7.42187500e-02, -1.67968750e-01,\n",
       "       -1.08886719e-01, -9.04083252e-04, -7.37304688e-02,  5.49316406e-02,\n",
       "        6.03027344e-02,  8.39843750e-02,  9.17968750e-02, -1.32812500e-01,\n",
       "        1.22070312e-01, -8.78906250e-03,  1.19140625e-01, -1.94335938e-01,\n",
       "       -6.64062500e-02, -2.07031250e-01,  7.37304688e-02,  8.93554688e-02,\n",
       "        1.81884766e-02, -1.20605469e-01, -2.61230469e-02,  2.67333984e-02,\n",
       "        7.76367188e-02, -8.30078125e-02,  6.78710938e-02, -3.54003906e-02,\n",
       "        3.10546875e-01, -2.42919922e-02, -1.41601562e-01, -2.08007812e-01,\n",
       "       -4.57763672e-03, -6.54296875e-02, -4.95605469e-02,  2.22656250e-01,\n",
       "        1.53320312e-01, -1.38671875e-01, -5.24902344e-02,  4.24804688e-02,\n",
       "       -2.38281250e-01,  1.56250000e-01,  5.83648682e-04, -1.20605469e-01,\n",
       "       -9.22851562e-02, -4.44335938e-02,  3.61328125e-02, -1.86767578e-02,\n",
       "       -8.25195312e-02, -8.25195312e-02, -4.05273438e-02,  1.19018555e-02,\n",
       "        1.69921875e-01, -2.80761719e-02,  3.03649902e-03,  9.32617188e-02,\n",
       "       -8.49609375e-02,  1.57470703e-02,  7.03125000e-02,  1.62353516e-02,\n",
       "       -2.27050781e-02,  3.51562500e-02,  2.47070312e-01, -2.67333984e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"United_States\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXrS4yUCdwHn"
   },
   "source": [
    "#### 61. 単語の類似度\n",
    "“United States”と”U.S.”のコサイン類似度を計算せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vC_oki8bdzH9"
   },
   "source": [
    "ここではsimilarityメソッドを利用します。2単語を指定すると、単語間のコサイン類似度を計算することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 901,
     "status": "ok",
     "timestamp": 1610700772937,
     "user": {
      "displayName": "yuki kato",
      "photoUrl": "",
      "userId": "08430221927587521076"
     },
     "user_tz": -540
    },
    "id": "IfaWllDHdqS6",
    "outputId": "44eb4507-f85b-4f60-c4e4-6353951f07e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73107743"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(\"United_States\", \"U.S.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrJp5G-7eAFr"
   },
   "source": [
    "#### 62. 類似度の高い単語10件\n",
    "“United States”とコサイン類似度が高い10語と，その類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjHeeTkkeFs7"
   },
   "source": [
    "ここではmost_similarメソッドを利用します。単語を指定すると、topnまでの類似度上位単語とその類似度を取得することができます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13554,
     "status": "ok",
     "timestamp": 1610700786695,
     "user": {
      "displayName": "yuki kato",
      "photoUrl": "",
      "userId": "08430221927587521076"
     },
     "user_tz": -540
    },
    "id": "kLJh0pjnd07a",
    "outputId": "3dbcc4b2-7e4b-4a15-e290-0d3b1fee8629"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Unites_States', 0.7877248525619507),\n",
       " ('Untied_States', 0.7541370987892151),\n",
       " ('United_Sates', 0.7400724291801453),\n",
       " ('U.S.', 0.7310774326324463),\n",
       " ('theUnited_States', 0.6404393911361694),\n",
       " ('America', 0.6178410053253174),\n",
       " ('UnitedStates', 0.6167312264442444),\n",
       " ('Europe', 0.6132988929748535),\n",
       " ('countries', 0.6044804453849792),\n",
       " ('Canada', 0.601906955242157)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"United_States\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4-XpfLUeMle"
   },
   "source": [
    "####　63. 加法構成性によるアナロジー\n",
    "“Spain”の単語ベクトルから”Madrid”のベクトルを引き，”Athens”のベクトルを足したベクトルを計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F19msMpEePuk"
   },
   "source": [
    "前問でも利用したmost_similarメソッドは、足すベクトルと引くベクトルをそれぞれ指定した上で、計算後のベクトルと類似度が高い単語を取得することができます。\n",
    "ここでは、問題文の指示に従い、Spain - Madrid + Athensのベクトルと類似度の高い単語を表示していますが、期待通りGreeceが1位に登場しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11430,
     "status": "ok",
     "timestamp": 1610700787055,
     "user": {
      "displayName": "yuki kato",
      "photoUrl": "",
      "userId": "08430221927587521076"
     },
     "user_tz": -540
    },
    "id": "ZPvHYUtmeGzm",
    "outputId": "8e6ff51d-b059-4938-a36b-6491adcc8396"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Greece', 0.6898480653762817),\n",
       " ('Aristeidis_Grigoriadis', 0.560684859752655),\n",
       " ('Ioannis_Drymonakos', 0.5552908778190613),\n",
       " ('Greeks', 0.545068621635437),\n",
       " ('Ioannis_Christou', 0.5400862097740173),\n",
       " ('Hrysopiyi_Devetzi', 0.5248445272445679),\n",
       " ('Heraklio', 0.5207759737968445),\n",
       " ('Athens_Greece', 0.516880989074707),\n",
       " ('Lithuania', 0.5166865587234497),\n",
       " ('Iraklion', 0.5146791338920593)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = model[\"Spain\"] - model[\"madrid\"] + model[\"Athens\"]\n",
    "model.most_similar(positive=[\"Spain\", \"Athens\"], negative=[\"Madrid\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCLQAGsneU7L"
   },
   "source": [
    "#### 64. アナロジーデータでの実験\n",
    "単語アナロジーの評価データをダウンロードし，vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語)を計算し，そのベクトルと類似度が最も高い単語と，その類似度を求めよ．求めた単語と類似度は，各事例の末尾に追記せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPAOsHtdeX2A"
   },
   "source": [
    "指定のデータをダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9151,
     "status": "ok",
     "timestamp": 1610700787055,
     "user": {
      "displayName": "yuki kato",
      "photoUrl": "",
      "userId": "08430221927587521076"
     },
     "user_tz": -540
    },
    "id": "A9wzmEtoeTF9",
    "outputId": "3934bcf3-e3be-4ba7-e26b-0985758e55dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-12 11:30:41--  http://download.tensorflow.org/data/questions-words.txt\n",
      "Resolving download.tensorflow.org (download.tensorflow.org)... 142.251.42.176, 2404:6800:4004:810::2010\n",
      "Connecting to download.tensorflow.org (download.tensorflow.org)|142.251.42.176|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 603955 (590K) [text/plain]\n",
      "Saving to: ‘questions-words.txt’\n",
      "\n",
      "questions-words.txt 100%[===================>] 589.80K   457KB/s    in 1.3s    \n",
      "\n",
      "2021-12-12 11:30:44 (457 KB/s) - ‘questions-words.txt’ saved [603955/603955]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://download.tensorflow.org/data/questions-words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7328,
     "status": "ok",
     "timestamp": 1610700787287,
     "user": {
      "displayName": "yuki kato",
      "photoUrl": "",
      "userId": "08430221927587521076"
     },
     "user_tz": -540
    },
    "id": "idYQuYQjeZrC",
    "outputId": "434efa2b-07ba-427a-8f69-f27a46440eb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": capital-common-countries\n",
      "Athens Greece Baghdad Iraq\n",
      "Athens Greece Bangkok Thailand\n",
      "Athens Greece Beijing China\n",
      "Athens Greece Berlin Germany\n",
      "Athens Greece Bern Switzerland\n",
      "Athens Greece Cairo Egypt\n",
      "Athens Greece Canberra Australia\n",
      "Athens Greece Hanoi Vietnam\n",
      "Athens Greece Havana Cuba\n"
     ]
    }
   ],
   "source": [
    "# 先頭10行の確認\n",
    "!head -10 questions-words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HP3L2oaBeead"
   },
   "source": [
    "1行ずつ読込み、指定の単語と類似度を計算した上で整形したデータを出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./questions-words.txt\", \"r\") as f1, open(\n",
    "    \"./questions-words-add.txt\", \"w\"\n",
    ") as f2:\n",
    "    for line in f1:  # f1から1行ずつ読込み、求めた単語と類似度を追加してf2に書込む\n",
    "        line = line.split()\n",
    "        if line[0] == \":\":\n",
    "            category = line[1]\n",
    "        else:\n",
    "            word, cos = model.most_similar(\n",
    "                positive=[line[1], line[2]], negative=[line[0]], topn=1\n",
    "            )[0]\n",
    "            f2.write(\" \".join([category] + line + [word, str(cos) + \"\\n\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital-common-countries Athens Greece Baghdad Iraq Iraqi 0.635187029838562\n",
      "capital-common-countries Athens Greece Bangkok Thailand Thailand 0.7137669324874878\n",
      "capital-common-countries Athens Greece Beijing China China 0.7235778570175171\n",
      "capital-common-countries Athens Greece Berlin Germany Germany 0.6734622716903687\n",
      "capital-common-countries Athens Greece Bern Switzerland Switzerland 0.4919748306274414\n",
      "capital-common-countries Athens Greece Cairo Egypt Egypt 0.7527808547019958\n",
      "capital-common-countries Athens Greece Canberra Australia Australia 0.583732545375824\n",
      "capital-common-countries Athens Greece Hanoi Vietnam Viet_Nam 0.6276341676712036\n",
      "capital-common-countries Athens Greece Havana Cuba Cuba 0.6460990905761719\n",
      "capital-common-countries Athens Greece Helsinki Finland Finland 0.68999844789505\n"
     ]
    }
   ],
   "source": [
    "!head -10 questions-words-add.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuxVtF_efRhj"
   },
   "source": [
    "#### 65. アナロジータスクでの正解率\n",
    "64の実行結果を用い，意味的アナロジー（semantic analogy）と文法的アナロジー（syntactic analogy）の正解率を測定せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndkMw7uMfWRb"
   },
   "source": [
    "対応するカテゴリごとにそれぞれ計算します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JiCnCWXvelv6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "意味的アナロジー正解率: 0.731\n",
      "文法的アナロジー正解率: 0.740\n"
     ]
    }
   ],
   "source": [
    "with open(\"./questions-words-add.txt\", \"r\") as f:\n",
    "    sem_cnt = 0\n",
    "    sem_cor = 0\n",
    "    syn_cnt = 0\n",
    "    syn_cor = 0\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        if not line[0].startswith(\"gram\"):\n",
    "            sem_cnt += 1\n",
    "            if line[4] == line[5]:\n",
    "                sem_cor += 1\n",
    "        else:\n",
    "            syn_cnt += 1\n",
    "            if line[4] == line[5]:\n",
    "                syn_cor += 1\n",
    "\n",
    "print(f\"意味的アナロジー正解率: {sem_cor/sem_cnt:.3f}\")\n",
    "print(f\"文法的アナロジー正解率: {syn_cor/syn_cnt:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLiHie_jgCrz"
   },
   "source": [
    "#### 66. WordSimilarity-353での評価\n",
    "The WordSimilarity-353 Test Collectionの評価データをダウンロードし，単語ベクトルにより計算される類似度のランキングと，人間の類似度判定のランキングの間のスピアマン相関係数を計算せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioyXzEzmgFYw"
   },
   "source": [
    "このデータは、単語のペアに対して人間が評価した類似度が付与されています。\n",
    "それぞれのペアに対して単語ベクトルの類似度を計算し、両者のスピアマン順位相関係数を計算します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "MHpixX5VfX5x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-12 13:09:34--  http://www.gabrilovich.com/resources/data/wordsim353/wordsim353.zip\n",
      "Resolving www.gabrilovich.com (www.gabrilovich.com)... 208.97.177.37\n",
      "Connecting to www.gabrilovich.com (www.gabrilovich.com)|208.97.177.37|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://gabrilovich.com/resources/data/wordsim353/wordsim353.zip [following]\n",
      "--2021-12-12 13:09:35--  https://gabrilovich.com/resources/data/wordsim353/wordsim353.zip\n",
      "Resolving gabrilovich.com (gabrilovich.com)... 208.97.177.37\n",
      "Connecting to gabrilovich.com (gabrilovich.com)|208.97.177.37|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 23257 (23K) [application/zip]\n",
      "Saving to: ‘wordsim353.zip’\n",
      "\n",
      "wordsim353.zip      100%[===================>]  22.71K   133KB/s    in 0.2s    \n",
      "\n",
      "2021-12-12 13:09:37 (133 KB/s) - ‘wordsim353.zip’ saved [23257/23257]\n",
      "\n",
      "Archive:  wordsim353.zip\n",
      "  inflating: combined.csv            \n",
      "  inflating: set1.csv                \n",
      "  inflating: set2.csv                \n",
      "  inflating: combined.tab            \n",
      "  inflating: set1.tab                \n",
      "  inflating: set2.tab                \n",
      "  inflating: instructions.txt        \n"
     ]
    }
   ],
   "source": [
    "!wget http://www.gabrilovich.com/resources/data/wordsim353/wordsim353.zip\n",
    "!unzip wordsim353.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 1,Word 2,Human (mean)\n",
      "love,sex,6.77\n",
      "tiger,cat,7.35\n",
      "tiger,tiger,10.00\n",
      "book,paper,7.46\n",
      "computer,keyboard,7.62\n",
      "computer,internet,7.58\n",
      "plane,car,5.77\n",
      "train,car,6.31\n",
      "telephone,communication,7.50\n"
     ]
    }
   ],
   "source": [
    "!head -10 './combined.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 国名の取得\n",
    "countries = set()\n",
    "with open(\"./questions-words-add.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        if line[0] in [\"capital-common-countries\", \"capital-world\"]:\n",
    "            countries.add(line[2])\n",
    "        elif line[0] in [\"currency\", \"gram6-nationality-adjective\"]:\n",
    "            countries.add(line[1])\n",
    "countries = list(countries)\n",
    "\n",
    "# 単語ベクトルの取得\n",
    "countries_vec = [model[country] for country in countries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGNR5i6q4IAi"
   },
   "source": [
    "#### 67. k-meansクラスタリング\n",
    "国名に関する単語ベクトルを抽出し，k-meansクラスタリングをクラスタ数k=5として実行せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSVVbJgd4L_W"
   },
   "source": [
    "適当な国名リストの取得元が見つからなかったため、単語アナロジーの評価データから収集しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 国名の取得\n",
    "countries = set()\n",
    "with open(\"./questions-words-add.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        if line[0] in [\"capital-common-countries\", \"capital-world\"]:\n",
    "            countries.add(line[2])\n",
    "        elif line[0] in [\"currency\", \"gram6-nationality-adjective\"]:\n",
    "            countries.add(line[1])\n",
    "countries = list(countries)\n",
    "\n",
    "# 単語ベクトルの取得\n",
    "countries_vec = [model[country] for country in countries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NzVa-8R04Or7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster 0\n",
      "Nigeria, Ghana, Kenya, Malawi, Mauritania, Burundi, Niger, Mozambique, Gambia, Angola, Liberia, Zimbabwe, Sudan, Madagascar, Senegal, Botswana, Guinea, Mali, Zambia, Eritrea, Somalia, Gabon, Rwanda, Uganda, Libya, Algeria, Tunisia, Namibia\n",
      "cluster 1\n",
      "Norway, Liechtenstein, Greenland, Denmark, Iceland, Canada, Switzerland, Austria, Sweden, Finland, Germany, Belgium, Netherlands\n",
      "cluster 2\n",
      "Uzbekistan, Ukraine, Montenegro, Poland, Macedonia, Croatia, Belarus, Bulgaria, Albania, Russia, Latvia, Lithuania, Azerbaijan, Armenia, Slovakia, Turkmenistan, Cyprus, Kazakhstan, Slovenia, Estonia, Romania, Serbia, Kyrgyzstan, Moldova, Hungary, Georgia, Tajikistan\n",
      "cluster 3\n",
      "Syria, Taiwan, Bangladesh, Thailand, Portugal, Malaysia, Australia, Turkey, Qatar, France, Pakistan, Ireland, England, Indonesia, Israel, Oman, Malta, Lebanon, India, Europe, Iran, Spain, Afghanistan, Bahrain, China, USA, Iraq, Korea, Japan, Vietnam, Morocco, Italy, Egypt, Jordan, Greece\n",
      "cluster 4\n",
      "Cambodia, Bahamas, Dominica, Belize, Samoa, Uruguay, Venezuela, Chile, Cuba, Tuvalu, Nepal, Guyana, Jamaica, Mexico, Nicaragua, Brazil, Philippines, Peru, Bhutan, Laos, Ecuador, Argentina, Suriname, Colombia, Honduras, Fiji\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# k-meansクラスタリング\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(countries_vec)\n",
    "for i in range(5):\n",
    "    cluster = np.where(kmeans.labels_ == i)[0]\n",
    "    print(\"cluster\", i)\n",
    "    print(\", \".join([countries[k] for k in cluster]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlQITWXM4VFM"
   },
   "source": [
    "#### 68. Ward法によるクラスタリング\n",
    "国名に関する単語ベクトルに対し，Ward法による階層型クラスタリングを実行せよ．さらに，クラスタリング結果をデンドログラムとして可視化せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bgqxdFvw4Qc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster 0\n",
      "Syria, Qatar, Israel, Oman, Malta, Lebanon, Iran, Afghanistan, Bahrain, Iraq, Libya, Morocco, Egypt, Jordan, Tunisia\n",
      "cluster 1\n",
      "Uzbekistan, Ukraine, Montenegro, Poland, Turkey, Macedonia, Croatia, Belarus, Bulgaria, Albania, Russia, Latvia, Lithuania, Azerbaijan, Armenia, Slovakia, Turkmenistan, Cyprus, Kazakhstan, Slovenia, Estonia, Romania, Serbia, Kyrgyzstan, Moldova, Hungary, Georgia, Tajikistan, Greece\n",
      "cluster 2\n",
      "Nigeria, Ghana, Kenya, Malawi, Mauritania, Burundi, Niger, Mozambique, Gambia, Angola, Liberia, Zimbabwe, Sudan, Madagascar, Senegal, Botswana, Guinea, Mali, Zambia, Eritrea, Somalia, Gabon, Rwanda, Uganda, Algeria, Namibia\n",
      "cluster 3\n",
      "Bahamas, Dominica, Belize, Samoa, Uruguay, Venezuela, Chile, Cuba, Guyana, Jamaica, Mexico, Nicaragua, Brazil, Peru, Ecuador, Argentina, Suriname, Colombia, Honduras, Fiji\n",
      "cluster 4\n",
      "Cambodia, Taiwan, Bangladesh, Thailand, Portugal, Malaysia, Norway, Australia, Liechtenstein, Greenland, France, Tuvalu, Denmark, Iceland, Canada, Switzerland, Nepal, Pakistan, Austria, Ireland, England, Indonesia, Sweden, India, Europe, Spain, China, USA, Korea, Japan, Philippines, Bhutan, Laos, Finland, Vietnam, Italy, Germany, Belgium, Netherlands\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# k-meansクラスタリング\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(countries_vec)\n",
    "for i in range(5):\n",
    "    cluster = np.where(kmeans.labels_ == i)[0]\n",
    "    print(\"cluster\", i)\n",
    "    print(\", \".join([countries[k] for k in cluster]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLhpbTBG4ZMs"
   },
   "source": [
    "#### 69. t-SNEによる可視化\n",
    "国名に関する単語ベクトルのベクトル空間をt-SNEで可視化せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1wuYUqm4b-5"
   },
   "source": [
    "t-SNEで単語ベクトルを2次元に圧縮し、散布図で可視化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bhtsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xoCrvNdy4c_T"
   },
   "outputs": [],
   "source": [
    "# import bhtsne\n",
    "\n",
    "# embedded = bhtsne.tsne(np.array(countries_vec).astype(np.float64), dimensions=2, rand_seed=123)\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.scatter(np.array(embedded).T[0], np.array(embedded).T[1])\n",
    "# for (x, y), name in zip(embedded, countries):\n",
    "#     plt.annotate(name, (x, y))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lc_ipsRr4eh1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOgwDECjy7UgC66VszsvmS1",
   "collapsed_sections": [],
   "name": "言語処理100本ノック_2020_7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
